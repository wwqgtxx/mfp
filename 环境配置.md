## 01-系统安装

这里采用debian10.7.0系统，安装仅勾选OpenSSH服务

所有用户名和密码均采用wwq

### 安装完后配置sudo

```bash
su -
adduser wwq sudo
reboot
```

### 配置sudo不用输入密码

执行命令

```bash
sudo nano /etc/sudoers
```

找到

```ini
%sudo ALL=(ALL:ALL) ALL
```

这一行修改为

```ini
%sudo ALL=(ALL:ALL) NOPASSWD:ALL
```

### 克隆虚拟机

克隆四份分别为：hadoop-01 hadoop-02 hadoop-03 hadoop-04

### 配置网络

执行命令

```bash
sudo nano /etc/network/interfaces
```

修改后几行为

```ini
allow-hotplug ens33
auto ens33
iface ens33 inet dhcp

allow-hotplug ens34
auto ens34
iface ens34 inet static
address 192.168.23.101
netmask 255.255.255.0
#gateway 192.168.23.1
#dns-nameservers 192.168.23.1
```

> 这里我的虚拟机采用的是双网卡
>
> 其他机器对应ip地址为102、103、104

执行

```bash
sudo systemctl restart networking
ip addr
```

### 设置hosts

执行命令

```bash
sudo nano /etc/hosts
```

添加

```ini
192.168.23.101 hadoop-01
192.168.23.102 hadoop-02
192.168.23.103 hadoop-03
192.168.23.104 hadoop-04
```

每台执行

```bash
sudo hostnamectl set-hostname hadoop-01
```

> 对应机器修改为02、0304

### 设置时区

```bash
sudo timedatectl set-timezone Asia/Shanghai
```

### 配置免密登录

```bash
ssh-keygen -t rsa
ssh-copy-id -i .ssh/id_rsa.pub hadoop-01
ssh-copy-id -i .ssh/id_rsa.pub hadoop-02
ssh-copy-id -i .ssh/id_rsa.pub hadoop-03
ssh-copy-id -i .ssh/id_rsa.pub hadoop-04
```

> `su -`切换到root账户之后再执行一遍
>
> 执行之前先临时将/etc/ssh/sshd_config中`PermitRootLogin prohibit-password`改为`PermitRootLogin yes`（记得`systemctl restart sshd`）
>
> 加上`-i .ssh/id_rsa.pub`是为了防止MobaXterm放置本机公钥（MobaXterm所在机器的公钥）

### 配置文件同步脚本

```bash
sudo apt install rsync
mkdir -p ~/bin
nano ~/bin/xsync
chmod +x ~/bin/xsync
```

`~/bin/xsync`:

```bash
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi
#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname
#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir
#4 获取当前用户名称
user=`whoami`
#5 循环
for host in `seq -s ' ' -f %02g 1 4`; do
    echo ------------------- hadoop-$host --------------
    rsync -rvl $pdir/$fname $user@hadoop-$host:$pdir
done
```

修改`/etc/profile`，添加如下：

```bash
if [ -d $HOME/bin ]; then
    export PATH="$HOME/bin:$PATH"
fi
```

> 该设置仅在debian系统中需要设置，在ubuntu和centos中均不需要

### 配置同步执行脚本

`~/bin/xcall.sh`:

```bash
#!/bin/bash
for host in `seq -s ' ' -f %02g 1 4`; do
    echo "========== hadoop-$host =========="
    ssh -t hadoop-$host "$*"
done
```

### 目录准备

```bash
cd /opt
sudo mkdir module
sudo mkdir software
sudo chown wwq:wwq module/ software/
```



## 02-节点规划

| 服务名称 | 子服务            | hadoop-01 | hadoop-02 | hadoop-03 | hadoop-04 |
| -------- | ----------------- | --------- | --------- | --------- | --------- |
| HDFS     | NameNode          | √         |           |           |           |
|          | DataNode          | √         | √         | √         | √         |
|          | SecondaryNameNode |           |           |           | √         |
| Yarn     | NodeManage        | √         | √         | √         | √         |
|          | Resourcemanager   |           | √         |           |           |



## 03-JDK安装

### 上传并解压

上传`jdk-8u212-linux-x64.tar.gz`到`/opt/software`

```bash
#解压安装包
cd /opt/software/
tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
```

### 配置环境变量

```bash
sudo nano /etc/profile.d/env.sh
```

```shell
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
```

```bash
source /etc/profile.d/env.sh
java -version
```

需要在`~/.bashrc`开头加入

```bash
source /etc/profile.d/env.sh
```

### 分发文件

```bash
#分发JDK
xsync /opt/module/jdk1.8.0_212/
#分发环境文件
sudo /home/wwq/bin/xsync /etc/profile.d/env.sh
```



## 04-Hadoop安装

### 上传并解压

上传`hadoop-3.1.3.tar.gz`到`/opt/software`

```bash
#解压安装包
cd /opt/software/
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
ls /opt/module/hadoop-3.1.3
```

### 配置环境变量

```bash
sudo nano /etc/profile.d/env.sh
```
添加
```shell
##HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
```
分发文件
```bash
source /etc/profile.d/env.sh
sudo /home/wwq/bin/xsync /etc/profile.d/env.sh
```

### 配置集群

1）核心配置文件

配置core-site.xml

```bash
cd $HADOOP_HOME/etc/hadoop
nano $HADOOP_HOME/etc/hadoop/core-site.xml
```

文件内容如下：
```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop-01:9820</value>
</property>
<!-- 指定hadoop数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
</property>

<!-- 配置HDFS网页登录使用的静态用户为wwq -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>wwq</value>
</property>

<!-- 配置该wwq(superUser)允许通过代理访问的主机节点 -->
    <property>
        <name>hadoop.proxyuser.wwq.hosts</name>
        <value>*</value>
</property>
<!-- 配置该wwq(superUser)允许通过代理用户所属组 -->
    <property>
        <name>hadoop.proxyuser.wwq.groups</name>
        <value>*</value>
</property>
<!-- 配置该wwq(superUser)允许通过代理的用户-->
    <property>
        <name>hadoop.proxyuser.wwq.groups</name>
        <value>*</value>
</property>
</configuration>

```

2）HDFS配置文件

配置hdfs-site.xml

```bash
cd $HADOOP_HOME/etc/hadoop
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

文件内容如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- nn web端访问地址-->
	<property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop-01:9870</value>
    </property>
    
	<!-- 2nn web端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop-04:9868</value>
    </property>
    
    <!-- 测试环境指定HDFS副本的数量1 -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

```


3）YARN配置文件

配置yarn-site.xml

```bash
cd $HADOOP_HOME/etc/hadoop
nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

文件内容如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    
    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop-02</value>
    </property>
    
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    
    <!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
    
    <!-- yarn容器允许管理的物理内存大小 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>
    
    <!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>

```

4）MapReduce配置文件

配置mapred-site.xml

```bash
cd $HADOOP_HOME/etc/hadoop
nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

文件内容如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

```

5）配置workers

```bash
cd $HADOOP_HOME/etc/hadoop
nano $HADOOP_HOME/etc/hadoop/workers
```

在该文件中增加如下内容：

```
hadoop-01
hadoop-02
hadoop-03
hadoop-04
```

**注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。**

### 分发Hadoop

```bash
xsync /opt/module/hadoop-3.1.3/
```

### 群起集群

1）启动集群

​    （1）**如果集群是第一次启动**，需要在hadoop-01节点格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）

```bash
hdfs namenode -format
```

（2）启动HDFS

```bash
start-dfs.sh
```

（3）在配置了**ResourceManager的节点（hadoop-02）**启动YARN

```
ssh hadoop-02 start-yarn.sh
```

（4）Web端查看HDFS的Web页面：http://192.168.23.101:9870/

###  Hadoop群起脚本

`~/bin/hadoop.sh`:

```bash
#!/bin/bash
if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi
case $1 in
"start")
        echo " =================== 启动 hadoop集群 ==================="

        echo " --------------- 启动 hdfs ---------------"
        ssh hadoop-01 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
        echo " --------------- 启动 yarn ---------------"
        ssh hadoop-02 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
;;
"stop")
        echo " =================== 关闭 hadoop集群 ==================="

        echo " --------------- 关闭 yarn ---------------"
        ssh hadoop-02 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
        echo " --------------- 关闭 hdfs ---------------"
        ssh hadoop-01 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
    echo "Input Args Error..."
;;
esac
```

